{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bba0cdd-af22-491f-b692-ccc1b91dbfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(695, 2)\n",
      "                                             content  \\\n",
      "0  Adjuntas barrio-pueblo is a barrio and the adm...   \n",
      "1  Casa Pueblo is an environmental community orga...   \n",
      "2  Casa Pueblo is an environmental community orga...   \n",
      "\n",
      "                                            metadata  \n",
      "0  {'type': 'landmark', 'city': 'Adjuntas', 'name...  \n",
      "1  {'type': 'landmark', 'city': 'Adjuntas', 'name...  \n",
      "2  {'type': 'landmark', 'city': 'Adjuntas', 'name...  \n",
      "\n",
      " 0    Adjuntas barrio-pueblo is a barrio and the adm...\n",
      "Name: content, dtype: object\n",
      "\n",
      " 0    {'type': 'landmark', 'city': 'Adjuntas', 'name...\n",
      "Name: metadata, dtype: object\n",
      "\n",
      " landmark\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json('./datasets/fully_classified_puerto_rico_landmarks.json')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(3))\n",
    "print(\"\\n\", df.head(1).content)\n",
    "print(\"\\n\", df.head(1).metadata)\n",
    "print(\"\\n\", df.head(1).metadata[0]['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7ea74de-6356-4c07-951a-f8c681f54668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 695  Metadatas: 695\n"
     ]
    }
   ],
   "source": [
    "texts = df['content'].to_list()\n",
    "metadatas = df['metadata'].to_list()\n",
    "\n",
    "print(f'Texts: {len(texts)}  Metadatas: {len(metadatas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9240659c-32f5-410e-aab6-e9958af1cd5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 2)\n",
      "                                             content  \\\n",
      "0  As in most countries, this holiday is celebrat...   \n",
      "1  This is a Commonwealth of Puerto Rico official...   \n",
      "2       A federal and commonwealth official holiday.   \n",
      "\n",
      "                                            metadata  \n",
      "0  {'type': 'event', 'city': 'Puerto Rico', 'name...  \n",
      "1  {'type': 'event', 'city': 'Puerto Rico', 'name...  \n",
      "2  {'type': 'event', 'city': 'Puerto Rico', 'name...  \n",
      "\n",
      " 0    As in most countries, this holiday is celebrat...\n",
      "Name: content, dtype: object\n",
      "\n",
      " 0    {'type': 'event', 'city': 'Puerto Rico', 'name...\n",
      "Name: metadata, dtype: object\n",
      "\n",
      " event\n"
     ]
    }
   ],
   "source": [
    "### ---------\n",
    "### IMPORT EVENTS\n",
    "### ---------\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "events_df = pd.read_json('./datasets/puerto_rico_events.json')\n",
    "\n",
    "print(events_df.shape)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(events_df.head(3))\n",
    "print(\"\\n\", events_df.head(1).content)\n",
    "print(\"\\n\", events_df.head(1).metadata)\n",
    "print(\"\\n\", events_df.head(1).metadata[0]['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2055251-ee94-464e-8dec-4d41c74ca2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 29  Metadatas: 29\n"
     ]
    }
   ],
   "source": [
    "events_texts = events_df['content'].to_list()\n",
    "events_metadatas = events_df['metadata'].to_list()\n",
    "\n",
    "print(f'Texts: {len(events_texts)}  Metadatas: {len(events_metadatas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e9aab-d7a5-460f-808f-d25663020dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029a9b9-3add-418b-a8eb-d4e129194659",
   "metadata": {},
   "outputs": [],
   "source": [
    "### JOIN EVENTS AND LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6a52c03a-df9d-4ddd-967c-64d70ebd233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 724  Metadatas: 724\n"
     ]
    }
   ],
   "source": [
    "join_texts = events_texts + texts\n",
    "join_metadatas = events_metadatas + metadatas\n",
    "\n",
    "print(f'Texts: {len(join_texts)}  Metadatas: {len(join_metadatas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb120df5-587c-4d93-890a-352f73f347b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ee6bf-e5af-4802-ab29-6ca787847ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONLY FOR LANDMARKS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Init Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Chunk Size\n",
    "    chunk_overlap=200  # Overlap\n",
    ")\n",
    "\n",
    "# Divide text and propagate the metadata\n",
    "split_texts = []\n",
    "split_metadatas = []\n",
    "\n",
    "# To each original text and associate metadata\n",
    "for i, text in enumerate(texts):\n",
    "    # Divide - chunks\n",
    "    chunks = splitter.split_text(text)\n",
    "    split_texts.extend(chunks)\n",
    "    \n",
    "    # Propagate the same metadata to each chunk\n",
    "    split_metadatas.extend([metadatas[i]] * len(chunks))\n",
    "\n",
    "# Check results\n",
    "print(\"Chunks:\", split_texts)\n",
    "print(\"Metadata (propagate):\", split_metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e95b9-ea07-409a-b436-5133ef3934b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Metadata Dictionary to String (for embedding purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d4303fb-22f2-4633-a0c2-aaa082ee0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_metadata_to_str(metadata):\n",
    "    \"\"\"Convert any list-based metadata to a comma-separated string.\"\"\"\n",
    "    converted_metadata = {}\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, list):\n",
    "            # Join list into a comma-separated string\n",
    "            converted_metadata[key] = \", \".join(value)\n",
    "        else:\n",
    "            converted_metadata[key] = value\n",
    "    return converted_metadata\n",
    "\n",
    "# Convert each metadata entry to string (needed for embedding)\n",
    "converted_metadatas = [convert_metadata_to_str(meta) for meta in metadatas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93380aea-8c3e-4cab-a8bb-687d5ab9ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Import OpenAI API key from env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "openai_api_key = load_dotenv(dotenv_path='.env')\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1ceda050-d302-4dfd-8203-3e5c5d528d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5e0fdf02-c06e-47fe-95e6-3de3ddf0ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDb Storing process ...\n",
      "Inserting batch 1 of size 695...\n",
      "Chunks have been successfully split, vectorized, and stored in the vector database.\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "open_ai_embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002', api_key=openai_api_key)\n",
    "\n",
    "print('ChromaDb Storing process ...')\n",
    "\n",
    "# Batch insertion function to handle large datasets\n",
    "def batch_insert_texts(texts, metadatas, open_ai_embedding_model, persist_directory, batch_size=40000):\n",
    "    \"\"\"Insert texts and associated metadata in batches to avoid exceeding the maximum batch size.\"\"\"\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_metadatas = metadatas[i:i + batch_size]\n",
    "        print(f\"Inserting batch {i // batch_size + 1} of size {len(batch_texts)}...\")\n",
    "        Chroma.from_texts(\n",
    "            batch_texts,\n",
    "            embedding=open_ai_embedding_model,\n",
    "            metadatas=batch_metadatas,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "\n",
    "# Insert texts and metadata in batches\n",
    "batch_insert_texts(\n",
    "    texts=texts,\n",
    "    metadatas=converted_metadatas,\n",
    "    open_ai_embedding_model=open_ai_embedding_model,\n",
    "    persist_directory=\"./chroma_db_tourism_v2\",\n",
    "    batch_size=40000  # Adjust batch size if necessary\n",
    ")\n",
    "\n",
    "print(\"Chunks have been successfully split, vectorized, and stored in the vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6781a-2b59-4b3c-8c28-82020a74e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Vector\n",
    "# vectorstore = Chroma.from_texts(\n",
    "#     texts=split_texts,\n",
    "#     embedding=embedding_function,  # Tu función de embedding\n",
    "#     metadatas=split_metadatas\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123ba8a-bd3e-415a-819c-24e62ba2ebde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869927c3-c25d-42be-a99f-2fe7f8084d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cd31211b-6013-4bfa-b5e2-6d7e7d626f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: As in most countries, this holiday is celebrated with gatherings and fireworks. Although illegal,celebratory gunfirehas sometimes led to injuries and even deaths on certain occasions.Official commonwealth and federal holiday.\n",
      "Metadata (propagate): {'type': 'event', 'city': 'Puerto Rico', 'name': \"New Year's Day\", 'categories': [], 'date': 'January 1', 'url': 'https://en.wikipedia.org/wiki/Public_holidays_in_Puerto_Rico', 'image_url': ''}\n"
     ]
    }
   ],
   "source": [
    "## ONLY FOR LANDMARKS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Init Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Chunk Size\n",
    "    chunk_overlap=200  # Overlap\n",
    ")\n",
    "\n",
    "# Divide text and propagate the metadata\n",
    "join_split_texts = []\n",
    "join_split_metadatas = []\n",
    "\n",
    "# To each original text and associate metadata\n",
    "for i, text in enumerate(join_texts):\n",
    "    # Divide - chunks\n",
    "    chunks = splitter.split_text(text)\n",
    "    join_split_texts.extend(chunks)\n",
    "    \n",
    "    # Propagate the same metadata to each chunk\n",
    "    join_split_metadatas.extend([join_metadatas[i]] * len(chunks))\n",
    "\n",
    "# Check results\n",
    "print(\"Chunks:\", join_split_texts[0])\n",
    "print(\"Metadata (propagate):\", join_split_metadatas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "981c563a-7437-4470-8efd-cb41bb440b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_metadata_to_str(metadata):\n",
    "    \"\"\"Convert any list-based metadata to a comma-separated string.\"\"\"\n",
    "    converted_metadata = {}\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, list):\n",
    "            # Join list into a comma-separated string\n",
    "            converted_metadata[key] = \", \".join(value)\n",
    "        else:\n",
    "            converted_metadata[key] = value\n",
    "    return converted_metadata\n",
    "\n",
    "# Convert each metadata entry to string (needed for embedding)\n",
    "converted_metadatas = [convert_metadata_to_str(meta) for meta in join_split_metadatas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f208cb7a-a79e-45e6-9001-cda149c03ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDb Storing process ...\n",
      "Inserting batch 1 of size 940...\n",
      "Chunks have been successfully split, vectorized, and stored in the vector database.\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "open_ai_embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002', api_key=openai_api_key)\n",
    "\n",
    "print('ChromaDb Storing process ...')\n",
    "\n",
    "# Batch insertion function to handle large datasets\n",
    "def batch_insert_texts(texts, metadatas, open_ai_embedding_model, persist_directory, batch_size=40000):\n",
    "    \"\"\"Insert texts and associated metadata in batches to avoid exceeding the maximum batch size.\"\"\"\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_metadatas = metadatas[i:i + batch_size]\n",
    "        print(f\"Inserting batch {i // batch_size + 1} of size {len(batch_texts)}...\")\n",
    "        Chroma.from_texts(\n",
    "            batch_texts,\n",
    "            embedding=open_ai_embedding_model,\n",
    "            metadatas=batch_metadatas,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "# Insert texts and metadata in batches\n",
    "batch_insert_texts(\n",
    "    texts=join_split_texts,\n",
    "    metadatas=converted_metadatas,\n",
    "    open_ai_embedding_model=open_ai_embedding_model,\n",
    "    persist_directory=\"./chroma_db_tourism_v2\",\n",
    "    batch_size=40000  # Adjust batch size if necessary\n",
    ")\n",
    "\n",
    "print(\"Chunks have been successfully split, vectorized, and stored in the vector database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
